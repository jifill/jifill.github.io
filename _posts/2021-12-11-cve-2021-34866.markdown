---
layout: post
title:  "Exploiting CVE-2021-34866 as an unprivileged user"
date:   2021-12-11 07:33:46 +0000
---

On October 13th 2021, a [zdi advisory][zdi-advisory] was published for [cve-2021-34866][cve-description] and was described as an "eBPF Type Confusion Privilege Escalation Vulnerability". I happened to come across this advisory a few days after and felt that this would be a good first cve for which to write an LPE exploit that would take a completely unprivileged user to root.

At this point, I hadn't written any bpf related exploits, so I spent a few days to a week looking into bpf related information and bpf related ctf challenges in preparation for creating a crashing POC for the vulnerability.

A few weeks later, someone else had released an exploit and writeup for the same cve which required specific capabilities in order to work. This was quite unfortunate as I already had a leak/kaslr-bypass by this point, but seeing as this person's exploit still required additional capabilities, I continued to work on my own exploit as my goal had been a completely unprivileged LPE from the beginning. 

After about two more weeks of struggle, I finally managed to finish the exploit despite the many times that it seemed like all hope was lost over the month or so that I had been working on it.


Below, I'll briefly describe the vulnerability and then proceed to describe various parts of my exploit which came together to produce a working LPE.


## The vulnerability

The commit containing the patch can be found [here][vulnerability-patch].

{% highlight diff %}
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index 381d3d6f24bcb..49f07e2bf23b9 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -5150,8 +5150,6 @@ static int check_map_func_compatibility(struct bpf_verifier_env *env,
 	case BPF_MAP_TYPE_RINGBUF:
 		if (func_id != BPF_FUNC_ringbuf_output &&
 		    func_id != BPF_FUNC_ringbuf_reserve &&
-		    func_id != BPF_FUNC_ringbuf_submit &&
-		    func_id != BPF_FUNC_ringbuf_discard &&
 		    func_id != BPF_FUNC_ringbuf_query)
 			goto error;
 		break;
@@ -5260,6 +5258,12 @@ static int check_map_func_compatibility(struct bpf_verifier_env *env,
 		if (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)
 			goto error;
 		break;
+	case BPF_FUNC_ringbuf_output:
+	case BPF_FUNC_ringbuf_reserve:
+	case BPF_FUNC_ringbuf_query:
+		if (map->map_type != BPF_MAP_TYPE_RINGBUF)
+			goto error;
+		break;
 	case BPF_FUNC_get_stackid:
 		if (map->map_type != BPF_MAP_TYPE_STACK_TRACE)
 			goto error;

{% endhighlight %}


The vulnerability is present in the check_map_func_compatibility() function, and it arose due to missing checks regarding the use of 3 specific bpf helper functions that are intended only to be used with a bpf ringbuf map. These helper functions are [bpf_ringbuf_reserve()][internal_ringbuf-reserve_function], [bpf_ringbuf_query()][ringbuf-query_function], and bpf_ringbuf_output(). The checks are intended to be present in a pair of switch statements, which first attempt to ensure that a map is only being used with specific helper functions (for specific types of maps) and the second switch statement attempts to ensure that a helper function is only being used with specific maps (for certain helper functions). However, due to lack of checks for the bpf ringbuf helper functions, we are able to use the aforementioned helper functions with bpf maps that are not bpf ringbuf. 

The maps available to us (to take advantage of these missing checks) are constrained to be only those that are not explicitly checked in check_map_func_compatibility() of course, so that made it somewhat straightforward to narrow down which maps we would be able to use the ringbuf helper functions on.

From this group of maps, some of them are restricted and only available to users with additional capabilities. A few of the maps that are actually available to unprivileged users however, are arraymaps and hashmaps.

When comparing the structs for arraymaps, ringbufmaps, and hashmaps, all of struct bpf_ringbuf_map (the struct for ringbuf maps), [struct bpf_htab][struct-bpf-htab] (the struct for hashmaps), and struct bpf_array (the struct for arraymaps) contain an embedded struct bpf_map followed additional members. Specifically, I took note of the field directly after the embedded bpf_map struct in each of the map super structs. For struct bpf_ringbuf_map there is a pointer to [struct bpf_ringbuf][bpf_ringbuf-struct], for struct bpf_htab there is a pointer to [struct bucket][struct-bucket], and for struct bpf_array there is a dword that represents the element size followed by another dword that represents a mask for index values. What is of importance here is that the bpf_helper functions we're considering all make use of the struct bpf_ringbuf pointer included in the bpf_ringbuf_map struct, so the member directly after the embedded bpf_map struct will be interpreted as a struct bpf_ringbuf.

{% highlight c %}
struct bpf_array {
	struct bpf_map map;
	u32 elem_size;
	u32 index_mask;
	[...]
};
{% endhighlight %}

{% highlight c %}
struct bpf_htab {
	struct bpf_map map;
	struct bucket *buckets;
	[...]
};
{% endhighlight %}

{% highlight c %}
struct bpf_ringbuf_map {
	struct bpf_map map;
	struct bpf_ringbuf *rb;
};
{% endhighlight %}




This means that we should be able to easily cause a pagefault and get a segfault or OOPS if we create an arraymap with specific values for the elem_size and index_mask fields. This is exactly what I did for my initial crashing POC in order to demonstrate that I understood the bug correctly.

For the actual exploit however, this meant that I would be exploring the interpretation of struct bpf_htab as a struct bpf_ringbuf.
This exploit was developed on a build of version 5.13.10 of the linux kernel, so all offsets described henceforth are in regards to that specific build. 

For hashmaps, the pointer to struct bucket is actually an array (of bucket structs) that constitutes a hashtable, with each bucket containing the head of a linked list along with a spinlock. When creating a hashmap, the user can control the size of the bucket struct array (the hashtable) by way of the max_entries parameter as it specifies the amount of bucket structs that will make up the array. In my environment, the size of struct bucket was 0x10 bytes and the first element in the struct was a pointer to the [htab_elem struct][htab-elem-struct] that was the head of that bucket's list. 

Going back to the helper functions for a bit, the first relevant helper function is bpf_ringbuf_query(). The gist of this function is that it allows the caller to read the producer_pos, consumer_pos, and mask fields of the internal struct bpf_ringbuf of the specified bpf_ringbuf_map. 

{% highlight c %}
BPF_CALL_2(bpf_ringbuf_query, struct bpf_map *, map, u64, flags)
{
	struct bpf_ringbuf *rb;

	rb = container_of(map, struct bpf_ringbuf_map, map)->rb;

	switch (flags) {
	case BPF_RB_AVAIL_DATA:
		return ringbuf_avail_data_sz(rb);
	case BPF_RB_RING_SIZE:
		return rb->mask + 1;
	case BPF_RB_CONS_POS:
		return smp_load_acquire(&rb->consumer_pos);
	case BPF_RB_PROD_POS:
		return smp_load_acquire(&rb->producer_pos);
	default:
		return 0;
	}
}
{% endhighlight %}


One notable thing about bpf_ringbuf struct is that the producer_pos,consumer_pos,and data fields are all literally on their own pages. This will be important later, when we get to our relative write. For now, the first task is defeating kaslr of course, as it is almost always assumed to be enabled when attacking the kernel. Of course, all we really need to defeat kaslr is to leak a kernel text address (or any address in the kernel image really). Luckily for us, due to the type confusion caused by this vulnerability, we can call bpf_ringbuf_query() on maps that are not ringbuf maps (i.e. hashmaps) and read qword values at both one page and two pages after the internal bpf_ringbuf struct. Of course, we'll actually be using hashmaps, so we'll actually be reading a qword at one page and two pages after our bucket array when we call bpf_ringbuf_query() from inside of a bpf program. This is exactly how I obtained a text leak and computed the kaslr slide in my exploit, and can be seen inside of the source code of the exploit. There was another method that I came up with afterwards to find leaks, but it was more complicated, caused more corruption, and the addresses that were found were not incredibly consistent. 


After finding a kaslr bypass, of course we're going to want a write primitive of some sort so that we can cause some actual corruption. The next function that concerns us is bpf_ringbuf_reserve(). This function is part of an api in which a producer can produce "records" of data within the ringbuffer that the bpf ringbuf map implements, and then either commit or discard that record. The gist of this function is that it reserves one of these records for the caller based on the arguments provided. Once the record is committed, a consumer can consume that record and use it for whatever purpose. 

{% highlight c %}
struct bpf_ringbuf {
	wait_queue_head_t waitq;
	struct irq_work work;
	u64 mask;
	struct page **pages;
	int nr_pages;
	spinlock_t spinlock ____cacheline_aligned_in_smp;
	/* Consumer and producer counters are put into separate pages to allow
	 * mapping consumer page as r/w, but restrict producer page to r/o.
	 * This protects producer position from being modified by user-space
	 * application and ruining in-kernel position tracking.
	 */
	unsigned long consumer_pos __aligned(PAGE_SIZE);
	unsigned long producer_pos __aligned(PAGE_SIZE);
	char data[] __aligned(PAGE_SIZE);
};
{% endhighlight %}

As it pertains to us, the return value from the bpf_ringbuf_reserve() helper is a pointer to the data of the reserved record inside of the ringbuf map. Specifically, it will be in the data buffer (the data member of struct bpf_ringbuf) offset by the previous position of producer counter. However, if we combine our vulnerability and use it with a bpf_hashmap, we can actually get a pointer to the kernel heap returned from bpf_ringbuf_reserve()! What's more is that the returned pointer is both readable and writable from within our bpf program! There are quite a few requirements regarding what the values of certain members need to be, and what others should ideally be however.

A few of them are:
1. The requested record size must be less than RINGBUF_MAX_RECORD_SZ
2. The spinlock field of the internal bpf_ringbuf struct must be 0 (so that the task does not block indefinitely when attempting to acquire the spinlock
3. The new producer position must be less than or equal to the mask field of the bpf_ringbuf struct

Another thing to note is that the record header is actually written to memory on each reserve call, so this will be smashing 8 bytes of kernel memory. 

The size is directly controlled from within a bpf program, so the first requirement is a non issue. The second requirement can be satisfied by a bit of spraying and praying. Specifically, you can spray objects set to 0 in hopes that it will overlap with the spinlock field of the bpf_ringbuf struct. In my specific case, the spinlock field was at offset 0x80 from the start of the bpf_ringbuf struct. So if we have our bucket array being 0x40 bytes in total, we can spray in the aforementioned manner in hopes of overlapping the spinlock with one of our objects.

![a view of struct bucket](/assets/cve-2021-34866/struct_bucket_size__ida.png)

![some offsets of the members of struct bpf_ringbuf](/assets/cve-2021-34866/struct_ringbuf_members__gdb.png)


When considering the third requirement, if we store an element in the hashtab we're using the ringbuf helpers on, a heap pointer for the htab_elem struct (an element in the htab) will be placed in one of the buckets in our bucket array (this is in the scenario that the hashmap was empty before). If it is placed in the bucket at offset 0x30 (the fourth bucket), we will have overlapped the pointer to the htab_elem struct with the mask field of the bpf_ringbuf struct, which will surely provide us with a large enough mask. Note that this also gives us a heap leak (if we choose to get the current value of the mask afterwards using bpf_ringbuf_query) which will be useful later. Being that we're discussing hashmaps, the index that an element is placed in within the bucket array (the hashtable) is determined via a hash of the (user-controlled) key. However, the hashmap is initialized with a random seed which attempts to randomize the index that a given key will be mapped to for each hashmap. Luckily for us, we can query the mask field (or in other words, bucket index 3) with our ringbuf helper and as such, we can try key after key and on each attempt, get the value to see if we've overlapped the mask field or not. When we have successfully overlapped the mask field, we will get a heap pointer when querying for `BPF_RB_RING_SIZE` (which returns the mask field + 1). At this point, we've satisfied the third requirement.

After this, we can write to (and read from) the ringbuf record from a bpf program and thus have a relative read/write primitive on the heap. I chose to look for tty_structs as it is a fairly straightforward way to achieve arbitrary read/write. Particularly, I repeatedly scanned half a page of "ringbuf" record memory (really just pages after the type confused hashmaps bucket array /hashtable) searching for ptm_unix98_ops (the operations table for tty_struct depending on how they are created) and replaced with a heap pointer under my control.

Now, the way I obtained this heap pointer is by again abusing the features of hashmaps in combination with the vulnerability. Hashmaps have a flag that allows for elements to be preallocated, and essentially recycled. With this flag enabled, if an element is deleted from a hashmap, it is actually stored on a hashmap-internal freelist as opposed to being returned to the slab or slub allocator, etc. 
{% highlight c %}
//sys_bpf(delete_elem) -> htab_map_delete_elem() -> free_htab_elem()

static void free_htab_elem(struct bpf_htab *htab, struct htab_elem *l)
{
	htab_put_fd_value(htab, l);

	if (htab_is_prealloc(htab)) {
		__pcpu_freelist_push(&htab->freelist, &l->fnode);
	} else {
		atomic_dec(&htab->count);
		l->htab = htab;
		call_rcu(&l->rcu, htab_elem_free_rcu);
	}
}
{% endhighlight %}

{% highlight c %}
static void htab_elem_free(struct bpf_htab *htab, struct htab_elem *l)
{
	if (htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH)
		free_percpu(htab_elem_get_ptr(l, htab->map.key_size));
	kfree(l);
}

static void htab_elem_free_rcu(struct rcu_head *head)
{
	struct htab_elem *l = container_of(head, struct htab_elem, rcu);
	struct bpf_htab *htab = l->htab;

	htab_elem_free(htab, l);
}
{% endhighlight %}

{% highlight c %}
//htab_map_update_elem() -> alloc_htab_elem()
static struct htab_elem *alloc_htab_elem(struct bpf_htab *htab, void *key,
					 void *value, u32 key_size, u32 hash,
					 bool percpu, bool onallcpus,
					 struct htab_elem *old_elem)
{
	u32 size = htab->map.value_size;
	bool prealloc = htab_is_prealloc(htab);
	struct htab_elem *l_new, **pl_new;
	void __percpu *pptr;

	if (prealloc) {
		if (old_elem) {
			[...]

		} else {
			struct pcpu_freelist_node *l;

			l = __pcpu_freelist_pop(&htab->freelist);
			if (!l)
				return ERR_PTR(-E2BIG);
			l_new = container_of(l, struct htab_elem, fnode);
		}
	}
	[...]
{% endhighlight %}



As such, if we add (allocate) an element in a hashtab that has preallcoation enabled (which was also 'empty' beforehand) and then delete the element, it will be placed on the hashmap's freelist. What's important here is that in this specific scenario, it will be the only such element on the hashmap's freelist! The consequence of this is that we can allocate an element, say E, free E, and reallocate E as many times as we want. When this is combined with our ability to find keys that will map to the (ringbuf stuct's ) mask field and read the mask field, we now know the address of a buffer whose contents we control on the kernel heap. I have yet to mention this explicitly, but we control the contents in this buffer because we can specify a value buffer when updating a hashmap with a new element. All in all, we can now repeatedly update the contents of a buffer kernel memory that resides at a known address which happens to be a pretty desirable primitive.


With this, we can simply forge our own [tty_operations struct][tty-operations-struct] in our controlled hashmap element buffer, and replace any instances of ptm_unix98_ops we find in memory with the address of our buffer. We can also have our bpf program write a value to an arraymap indicating when it has successfully done this. I simply combined this with the known technique of using small rop gadgets that abuse the fact that esi and rdx are completely attacker controlled when calling an ioctl function in order to create 4 byte readwhatwhere and writewhatwhere primitives. Once this is done, I'm able to call the ioctl function as many times as I want, and simply update the controlled buffer to change from "read mode" to "write mode".

{% highlight c %}
struct tty_struct {
	int	magic;
	struct kref kref;
	struct device *dev;	/* class device or NULL (e.g. ptys, serdev) */
	struct tty_driver *driver;
	const struct tty_operations *ops;
	[...]
};

struct tty_operations {

	[...]
	int  (*ioctl)(struct tty_struct *tty,
		    unsigned int cmd, unsigned long arg);
	[...]
};

{% endhighlight %}


{% highlight c %}
//0xffffffff8135c58c : mov eax, dword ptr [rdx] ; ret
//0xffffffff81105c28 : mov dword ptr [rdx], esi ; ret
{% endhighlight %}


At this point, all that is left is finding the file descriptor that corresponds to the corrupted tty_struct. Being that the read gadget I used loads a value from memory into eax, I simply kept file descriptors of all of the tty_structs that I sprayed in a table, and called their ioctl function with an attempt to read from a kernel address (that held a known value). The fd whose ioctl call doesn't return -1 is the fd that we can use for arbitrary read/write.

Once the controlled fd is found, the rest of the exploit is simply a matter of traversing the task list and writing to the cred struct of the current task. 


## Final Remarks

Although I chose to change the contents of my cred struct, there are of course many other options available to you once you have an arbitrary read/write primitive, and one could also choose to use a different struct/object for a memory corruption target from the relative read/write.

At a high level, the workflow of the exploit mostly consists of bypassing kaslr using the type confusion vulnerability, achieving a relative read/write on the kernel heap from a combination of the type confusion vulnerability and a heap groom and spray, and then turning that relative read/write into an arbitrary read/write.
 The reliability of the exploit is not 100% and can certainly be improved in a few ways, one of them being improving the heap groom and spray. If you're attempting to run the exploit in an environment other than the one provided in my repository, you'll likely run into problems as the exploit is highly specific to that specific environment as that is the context in which it was developed. I suppose that would mean that portability is another aspect of the exploit that can be improved. One thing I noticed was that simple calls to bpf_ringbuf_query() would simply crash instantly on some vulnerable versions such as v5.8 and v5.9 in my testing, so that probably means that there are some struct offset differences in those versions compared to the version my exploit runs on (v5.13.10) which gives yet another thing that can be worked on for those who are interested. 
Also, for clarification, the exploit being written to take a completely unprivileged user to having root privileges means that the exploit does not require `CAP_BPF`, `CAP_SYS_ADMIN`, or any other capability that a standard user would not have.

All in all, I enjoyed writing this exploit and look forward to doing similar work in the future. The source code for the exploit can be found [here][exploit-source].


[zdi-advisory]: https://www.zerodayinitiative.com/advisories/ZDI-21-1148/
[vulnerability-patch]: https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?id=5b029a32cfe4600f5e10e36b41778506b90fd4de
[ringbuf-query_function]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/ringbuf.c#L452
[internal_ringbuf-reserve_function]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/ringbuf.c#L305
[ringbuf-reserve-info-1]: https://fntlnz.wtf/post/bpf-ring-buffer-usage/
[ringbuf-discard_function]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/ringbuf.c#L411
[exploit-source-dummy-link]: https://github.com/jifill
[exploit-source]: https://github.com/jifill/linux_kernel_exploits/tree/master/cve-2021-34866
[tty-operations-struct]: https://elixir.bootlin.com/linux/v5.13.10/source/include/linux/tty_driver.h#L246
[htab-elem-struct]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/hashtab.c#L110
[bpf_ringbuf-struct]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/ringbuf.c#L33
[struct-bucket]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/hashtab.c#L81
[cve-description]: https://ubuntu.com/security/CVE-2021-34866
[struct-bpf-htab]: https://elixir.bootlin.com/linux/v5.13.10/source/kernel/bpf/hashtab.c#L92